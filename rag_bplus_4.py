import tempfile
import os
import streamlit as st
from concurrent.futures import ThreadPoolExecutor
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage, Document
from datetime import datetime
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë²¼ë½ì¹˜ê¸° ë„ìš°ë¯¸",
    page_icon="â³",
)

# íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_file(file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=file.name) as tmp_file:
        tmp_file.write(file.read())
        temp_file_path = tmp_file.name

    if file.name.endswith(".pdf"):
        loader = PyPDFLoader(temp_file_path)
    elif file.name.endswith(".docx"):
        loader = Docx2txtLoader(temp_file_path)
    elif file.name.endswith(".pptx"):
        loader = UnstructuredPowerPointLoader(temp_file_path)
    else:
        st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file.name}")
        return ""

    documents = loader.load_and_split()
    full_text = " ".join([doc.page_content for doc in documents])

    os.remove(temp_file_path)
    return full_text

# í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• 
def split_text_into_chunks(uploaded_text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,  # ì²­í¬ í¬ê¸° ì¡°ì •
        chunk_overlap=300  # ì²­í¬ ì¤‘ì²© ì¦ê°€
    )
    documents = [Document(page_content=text) for text in uploaded_text.values()]
    return text_splitter.split_documents(documents)

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def create_vectorstore(text_chunks):
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    return FAISS.from_documents(text_chunks, embeddings)

# í…ìŠ¤íŠ¸ ìš”ì•½ (ë¬¸ë‹¨ë³„ë¡œ ì²˜ë¦¬ ë° ê°œì„ )
def summarize_text(text_chunks, llm, max_summary_length=2000):
    def process_chunk(chunk):
        text = chunk.page_content
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ í•œêµ­ì–´ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ìš”ì•½ì€ ê°„ê²°í•˜ê³  í•µì‹¬ ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤."),
            HumanMessage(content=f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê° ìš”ì•½ í•­ëª©ì€ í•µì‹¬ ë‚´ìš©ì„ í¬í•¨í•˜ë©° '-'ë¡œ ì‹œì‘í•˜ë„ë¡ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{text}")
        ]
        response = llm(messages)
        return response.content.strip()

    with ThreadPoolExecutor(max_workers=4) as executor:  # ë³‘ë ¬ ì²˜ë¦¬
        summaries = list(executor.map(process_chunk, text_chunks))

    # ìš”ì•½ í•­ëª©ì„ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„í•˜ì—¬ ê²°í•©
    combined_summary = "\n\n".join([f"- {summary}" for summary in summaries])
    return combined_summary[:max_summary_length] + "..." if len(combined_summary) > max_summary_length else combined_summary

# ê³µë¶€ ë¡œë“œë§µ ìƒì„±
def create_study_roadmap(summary, llm, days_left, max_summary_length=2000):
    if len(summary) > max_summary_length:
        summary = summary[:max_summary_length] + "..."
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ìœ ëŠ¥í•œ ê³µë¶€ ë¡œë“œë§µ ì‘ì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤."),
        HumanMessage(content=f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {days_left}ì¼ ë™ì•ˆ í•œêµ­ ëŒ€í•™ìƒë“¤ì´ íš¨ê³¼ì ìœ¼ë¡œ ê³µë¶€í•  ìˆ˜ ìˆëŠ” ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {summary}
        """)
    ]
    response = llm(messages)
    return response.content

# ì˜ˆìƒ ë¬¸ì œ ìƒì„±
def generate_quiz_questions(summary, llm):
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ì˜ˆìƒ ë¬¸ì œë¥¼ ì‘ì„±í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."),
        HumanMessage(content=f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ë„ë¥¼ í‘œì‹œí•œ 10ê°œ ì´ìƒì˜ ì˜ˆìƒ ë¬¸ì œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {summary}
        - ì˜ˆìƒ ë¬¸ì œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        - ê° ë¬¸ì œì—ëŠ” ì¤‘ìš”ë„ë¥¼ 'ë†’ìŒ', 'ì¤‘ê°„', 'ë‚®ìŒ'ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
        """)
    ]
    response = llm(messages)
    return response.content

# Streamlit ì•± ì„¤ì •
def main():
    st.title("â³ ëŒ€í•™ìƒ ë²¼ë½ì¹˜ê¸° ë„ìš°ë¯¸")

    if "uploaded_text" not in st.session_state:
        st.session_state.uploaded_text = {}

    if "roadmap" not in st.session_state:
        st.session_state.roadmap = None

    if "quiz" not in st.session_state:
        st.session_state.quiz = None

    if "summary" not in st.session_state:
        st.session_state.summary = {}  # ìˆ˜ì •: ìš”ì•½ì„ íŒŒì¼ë³„ë¡œ ì €ì¥í•˜ë„ë¡ ì´ˆê¸°í™”

    with st.sidebar:
        uploaded_files = st.file_uploader("ğŸ“„ ê°•ì˜ ìë£Œ ì—…ë¡œë“œ", type=["pdf", "docx", "pptx"], accept_multiple_files=True)
        problem_files = st.file_uploader("â“ ë¬¸ì œ ì—…ë¡œë“œ (ì„ íƒ ì‚¬í•­)", type=["pdf", "docx", "pptx"], accept_multiple_files=True)  # ìƒˆë¡œìš´ ì—…ë¡œë“œë€
        openai_api_key = st.text_input("ğŸ”‘ OpenAI API í‚¤", type="password")
        exam_date = st.date_input("ğŸ“… ì‹œí—˜ ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”")
        process_button = st.button("ğŸš€ ë²¼ë½ì¹˜ê¸° ì‹œì‘í•˜ê¸°")
        create_summary = st.checkbox("í•µì‹¬ ìš”ì•½ ìƒì„±", value=True)
        create_roadmap = st.checkbox("ê³µë¶€ ë¡œë“œë§µ ìƒì„±", value=True)
        create_quiz = st.checkbox("ì˜ˆìƒ ë¬¸ì œ ìƒì„±", value=True)
        generate_related_problems = st.checkbox("ì—°ê´€ ë¬¸ì œ ìƒì„± (ë¬¸ì œ ì—…ë¡œë“œ ì‹œ)", value=True)  # ìƒˆë¡œìš´ ì²´í¬ë°•ìŠ¤ ì¶”ê°€
        selected_files = st.multiselect("ìš”ì•½ì„ í™•ì¸í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:", list(st.session_state.summary.keys()))
        
        # íŒŒì¼ë³„ ìš”ì•½ ê²°ê³¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)

    if process_button:
        if not openai_api_key:
            st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return
        if not uploaded_files:
            st.warning("ê°•ì˜ ìë£Œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
            return
        if not exam_date:
            st.warning("ì‹œí—˜ ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
            return

        # ì‹œí—˜ê¹Œì§€ ë‚¨ì€ ê¸°ê°„ ê³„ì‚°
        days_left = (exam_date - datetime.now().date()).days
        if days_left <= 0:
            st.warning("ì‹œí—˜ ë‚ ì§œëŠ” ì˜¤ëŠ˜ë³´ë‹¤ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤!")
            return

        # ì—…ë¡œë“œí•œ íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for file in uploaded_files:
            st.session_state.uploaded_text[file.name] = extract_text_from_file(file)

        # ë²¡í„° ì €ì¥ì†Œ ë° ìš”ì•½ ìƒì„±
        text_chunks = split_text_into_chunks(st.session_state.uploaded_text)
        vectorstore = create_vectorstore(text_chunks)
        llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4")  # GPT-4 ìœ ì§€

        # ì„ íƒì ìœ¼ë¡œ ë‹¨ê³„ ì‹¤í–‰
        if create_summary:
            st.session_state.summary = {
                file_name: summarize_text([chunk], llm)
                for file_name, chunk in zip(st.session_state.uploaded_text.keys(), text_chunks)
            }
        if create_roadmap:
            st.session_state.roadmap = create_study_roadmap(
                "\n".join(st.session_state.summary.values()), llm, days_left
            )
        if create_quiz:
            st.session_state.quiz = generate_quiz_questions(
                "\n".join(st.session_state.summary.values()), llm
            )
        # ë¬¸ì œ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ëŠ” í•¨ìˆ˜
        def truncate_text(text, max_tokens=2000):
            """í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í† í° ìˆ˜ë¡œ ì œí•œí•©ë‹ˆë‹¤."""
            tokens = text.split()  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
            if len(tokens) > max_tokens:
                return " ".join(tokens[:max_tokens])
            return text

    if st.session_state.uploaded_text:
        if create_summary and selected_files:
            st.subheader("ğŸ“Œ í•µì‹¬ ìš”ì•½")
            for selected_file in selected_files:
                st.markdown(f"**íŒŒì¼ëª…: {selected_file}**")
                st.markdown(st.session_state.summary[selected_file].replace("\n", "\n\n"))

        if create_roadmap:
            st.subheader("ğŸ“‹ ê³µë¶€ ë¡œë“œë§µ")
            st.markdown(st.session_state.roadmap)
        if create_quiz:
            st.subheader("â“ ì˜ˆìƒ ë¬¸ì œ")
            st.markdown(st.session_state.quiz)

        if problem_files and generate_related_problems:
            st.subheader("ğŸ“ ì¤‘ê°„&ê¸°ë§ ê´€ë ¨ ë¬¸ì œ ë° í’€ì´")
            for file in problem_files:
                problem_text = extract_text_from_file(file)
                if problem_text:
                    # ë¬¸ì œ í…ìŠ¤íŠ¸ ìë¥´ê¸°
                    truncated_problem_text = truncate_text(problem_text, max_tokens=2000)
                    messages = [
                        SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ë¬¸ì œ ìƒì„± ë° í’€ì´ ì‘ì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤."),
                        HumanMessage(content=f"""
                        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œ 5ê°œì˜ í•™ë¬¸ì  ìˆ˜ì¤€ì˜ ë¬¸ì œë¥¼ ì‘ì„±í•˜ê³ , ê° ë¬¸ì œì— ëŒ€í•œ ë³´ê¸°ì™€ í’€ì´ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
                        1. ë¬¸ì œ: [ë¬¸ì œ ë‚´ìš©]
                        ë³´ê¸°:
                        A) [ë³´ê¸°1]
                        B) [ë³´ê¸°2]
                        C) [ë³´ê¸°3]
                        D) [ë³´ê¸°4]
                        í’€ì´: [ë¬¸ì œ í’€ì´]
                        í…ìŠ¤íŠ¸:
                        {truncated_problem_text}
                        """)
                    ]
                    try:
                        # ëª¨ë¸ í˜¸ì¶œ
                        response = llm(messages)
                        generated_questions = response.content.split("\n\n")  # ë¬¸ì œì™€ í’€ì´ ë¶„ë¦¬
                        st.markdown(f"**íŒŒì¼ëª…: {file.name}**")

                        # ìƒì„±ëœ ë¬¸ì œ ì¶œë ¥
                        for idx, question_block in enumerate(generated_questions, 1):
                            # ë¬¸ì œì™€ í’€ì´ ë¶„ë¦¬
                            parts = question_block.split("í’€ì´:")
                            question_text = parts[0].strip()
                            solution_text = parts[1].strip() if len(parts) > 1 else "í’€ì´ ì—†ìŒ"

                            # ë¬¸ì œ ë³¸ë¬¸ ì¶œë ¥
                            st.markdown(f"### ë¬¸ì œ {idx}")
                            st.markdown(question_text.replace("ë³´ê¸°:", "\n\n**ë³´ê¸°:**"))

                            # í’€ì´ ìˆ¨ê¸°ê¸°
                            with st.expander(f"í’€ì´ ë³´ê¸° (ë¬¸ì œ {idx})"):
                                st.markdown(solution_text)
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")



        
if __name__ == "__main__":
    main()
