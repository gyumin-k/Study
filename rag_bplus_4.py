import tempfile
import os
import streamlit as st
from concurrent.futures import ThreadPoolExecutor
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage, Document
from datetime import datetime

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ë²¼ë½ì¹˜ê¸° ë„ìš°ë¯¸",
    page_icon="â³",
)

# íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_file(file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=file.name) as tmp_file:
        tmp_file.write(file.read())
        temp_file_path = tmp_file.name

    if file.name.endswith(".pdf"):
        loader = PyPDFLoader(temp_file_path)
    elif file.name.endswith(".docx"):
        loader = Docx2txtLoader(temp_file_path)
    elif file.name.endswith(".pptx"):
        loader = UnstructuredPowerPointLoader(temp_file_path)
    else:
        st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file.name}")
        return ""

    documents = loader.load_and_split()
    full_text = " ".join([doc.page_content for doc in documents])

    os.remove(temp_file_path)
    return full_text

# í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• 
def split_text_into_chunks(uploaded_text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=200
    )
    documents = [Document(page_content=text) for text in uploaded_text.values()]
    return text_splitter.split_documents(documents)

# í…ìŠ¤íŠ¸ ìš”ì•½
def summarize_text(text_chunks, llm, max_summary_length=2000):
    def process_chunk(chunk):
        text = chunk.page_content
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ í•œêµ­ì–´ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."),
            HumanMessage(content=f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}")
        ]
        response = llm(messages)
        return response.content

    with ThreadPoolExecutor(max_workers=4) as executor:
        summaries = list(executor.map(process_chunk, text_chunks))
    
    combined_summary = "\n".join(summaries)
    return combined_summary[:max_summary_length] + "..." if len(combined_summary) > max_summary_length else combined_summary

# ê³µë¶€ ë¡œë“œë§µ ìƒì„±
def create_study_roadmap(summary, llm, days_left, max_summary_length=2000):
    if len(summary) > max_summary_length:
        summary = summary[:max_summary_length] + "..."
    messages = [
        SystemMessage(content="ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ìœ ëŠ¥í•œ ê³µë¶€ ë¡œë“œë§µ ì‘ì„± ë„ìš°ë¯¸ì…ë‹ˆë‹¤."),
        HumanMessage(content=f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {days_left}ì¼ ë™ì•ˆ íš¨ê³¼ì ìœ¼ë¡œ ê³µë¶€í•  ìˆ˜ ìˆëŠ” ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {summary}
        """)
    ]
    response = llm(messages)
    return response.content

# ì˜ˆìƒ ë¬¸ì œ ìƒì„±
def generate_quiz_questions(text, llm, use_exam_format=False):
    system_message = "ë‹¹ì‹ ì€ í•œêµ­ ëŒ€í•™ìƒì„ ìœ„í•œ ì˜ˆìƒ ë¬¸ì œë¥¼ ì‘ì„±í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."
    if use_exam_format:
        system_message += " ê¸°ì¶œë¬¸ì œ í˜•ì‹ì„ ë°˜ì˜í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”."
    messages = [
        SystemMessage(content=system_message),
        HumanMessage(content=f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ë¬¸ì œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {text}
        - ê° ë¬¸ì œì—ëŠ” ì¤‘ìš”ë„ë¥¼ 'ë†’ìŒ', 'ì¤‘ê°„', 'ë‚®ìŒ'ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
        """)
    ]
    response = llm(messages)
    return response.content

# Streamlit ì•± ì„¤ì •
def main():
    st.title("â³ ëŒ€í•™ìƒ ë²¼ë½ì¹˜ê¸° ë„ìš°ë¯¸")

    if "lecture_text" not in st.session_state:
        st.session_state.lecture_text = {}

    if "exam_text" not in st.session_state:
        st.session_state.exam_text = {}

    if "summary" not in st.session_state:
        st.session_state.summary = {}

    if "roadmap" not in st.session_state:
        st.session_state.roadmap = None

    if "quiz" not in st.session_state:
        st.session_state.quiz = None

    with st.sidebar:
        lecture_files = st.file_uploader("ğŸ“„ ê°•ì˜ìë£Œ ì—…ë¡œë“œ", type=["pdf", "docx", "pptx"], accept_multiple_files=True, key="lecture")
        exam_files = st.file_uploader("ğŸ“„ ê¸°ì¶œë¬¸ì œ ì—…ë¡œë“œ", type=["pdf", "docx", "pptx"], accept_multiple_files=True, key="exam")
        openai_api_key = st.text_input("ğŸ”‘ OpenAI API í‚¤", type="password")
        exam_date = st.date_input("ğŸ“… ì‹œí—˜ ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”")
        process_button = st.button("ğŸš€ ë²¼ë½ì¹˜ê¸° ì‹œì‘í•˜ê¸°")

    if process_button:
        if not openai_api_key:
            st.warning("OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        if not lecture_files:
            st.warning("ê°•ì˜ìë£Œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
            return

        # ì‹œí—˜ê¹Œì§€ ë‚¨ì€ ê¸°ê°„ ê³„ì‚°
        days_left = (exam_date - datetime.now().date()).days
        if days_left <= 0:
            st.warning("ì‹œí—˜ ë‚ ì§œëŠ” ì˜¤ëŠ˜ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤!")
            return

        # ê°•ì˜ìë£Œì™€ ê¸°ì¶œë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for file in lecture_files:
            st.session_state.lecture_text[file.name] = extract_text_from_file(file)
        for file in exam_files:
            st.session_state.exam_text[file.name] = extract_text_from_file(file)

        # ê°•ì˜ìë£Œ ë° ê¸°ì¶œë¬¸ì œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        lecture_chunks = split_text_into_chunks(st.session_state.lecture_text)
        llm = ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4")

        # ìš”ì•½ ìƒì„±
        st.session_state.summary = summarize_text(lecture_chunks, llm)

        # ê³µë¶€ ë¡œë“œë§µ ìƒì„±
        st.session_state.roadmap = create_study_roadmap(
            st.session_state.summary, llm, days_left
        )

        # ì˜ˆìƒ ë¬¸ì œ ìƒì„±
        if st.session_state.exam_text:  # ê¸°ì¶œë¬¸ì œê°€ ìˆëŠ” ê²½ìš°
            exam_chunks = split_text_into_chunks(st.session_state.exam_text)
            all_text = "\n".join([chunk.page_content for chunk in lecture_chunks + exam_chunks])
            st.session_state.quiz = generate_quiz_questions(all_text, llm, use_exam_format=True)
        else:  # ê°•ì˜ìë£Œë§Œ ìˆëŠ” ê²½ìš°
            all_text = "\n".join([chunk.page_content for chunk in lecture_chunks])
            st.session_state.quiz = generate_quiz_questions(all_text, llm, use_exam_format=False)

    # ê²°ê³¼ ì¶œë ¥
    if st.session_state.summary:
        st.subheader("ğŸ“Œ í•µì‹¬ ìš”ì•½")
        st.markdown(st.session_state.summary)

    if st.session_state.roadmap:
        st.subheader("ğŸ“‹ ê³µë¶€ ë¡œë“œë§µ")
        st.markdown(st.session_state.roadmap)

    if st.session_state.quiz:
        st.subheader("â“ ì˜ˆìƒ ë¬¸ì œ")
        st.markdown(st.session_state.quiz)

if __name__ == "__main__":
    main()
